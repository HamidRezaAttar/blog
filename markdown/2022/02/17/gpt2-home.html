<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>GPT2-Home üè† | HamidReza Attar AI Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="GPT2-Home üè†" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Generate description for your home products using GPT2-Home!" />
<meta property="og:description" content="Generate description for your home products using GPT2-Home!" />
<link rel="canonical" href="https://hamidrezaattar.github.io/blog/markdown/2022/02/17/gpt2-home.html" />
<meta property="og:url" content="https://hamidrezaattar.github.io/blog/markdown/2022/02/17/gpt2-home.html" />
<meta property="og:site_name" content="HamidReza Attar AI Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-17T00:00:00-06:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="GPT2-Home üè†" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-02-17T00:00:00-06:00","datePublished":"2022-02-17T00:00:00-06:00","description":"Generate description for your home products using GPT2-Home!","headline":"GPT2-Home üè†","mainEntityOfPage":{"@type":"WebPage","@id":"https://hamidrezaattar.github.io/blog/markdown/2022/02/17/gpt2-home.html"},"url":"https://hamidrezaattar.github.io/blog/markdown/2022/02/17/gpt2-home.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://hamidrezaattar.github.io/blog/feed.xml" title="HamidReza Attar AI Blog" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">HamidReza Attar AI Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">GPT2-Home üè†</h1><p class="page-description">Generate description for your home products using GPT2-Home!</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-02-17T00:00:00-06:00" itemprop="datePublished">
        Feb 17, 2022
      </time>
       ‚Ä¢ <span class="read-time" title="Estimated read time">
    
    
      3 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#markdown">markdown</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h3"><a href="#"></a></li>
<li class="toc-entry toc-h1"><a href="#why-is-product-content-important">Why is product content important?</a>
<ul>
<li class="toc-entry toc-h2"><a href="#transformers">Transformers</a></li>
<li class="toc-entry toc-h2"><a href="#what-is-gpt-2">What is GPT-2</a></li>
<li class="toc-entry toc-h2"><a href="#what-is-gpt2-home">What is GPT2-Home?</a></li>
<li class="toc-entry toc-h2"><a href="#fastai-with-transformers">Fastai with ü§óTransformers</a></li>
<li class="toc-entry toc-h2"><a href="#-future-improvements">üìë Future Improvements</a></li>
</ul>
</li>
</ul><p>In a context where products need to be continuously fed with effective marketing descriptions to fit into a good SEO strategy, Artificial Intelligence with Natural Language Generation (NLG) technology appears to be an ideal solution to automatically generate quality and quantity content. This rapidly growing technology has become common in many industries and can become a real ally for your data-driven content marketing strategy.</p>

<blockquote>
  <p>The code used in this article can be found here ‚Äî <a href="https://docs.fast.ai/tutorial.transformers.html">How to fine-tune GPT-2</a>.
Also For testing GPT2-Home on a live <a href="https://huggingface.co/spaces/HamidRezaAttar/gpt2-home">Demo</a>, you can visit the model page on <a href="https://huggingface.co/HamidRezaAttar/gpt2-product-description-generator">Huggingface</a>.
Any question? Post a <a href="https://github.com/HamidRezaAttar/GPT2-Home">Github</a> issue.</p>
</blockquote>

<h3>
<a class="anchor" href="#" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://huggingface.co/HamidRezaAttar/gpt2-product-description-generator#how-to-use"></a>
</h3>

<h1 id="why-is-product-content-important">
<a class="anchor" href="#why-is-product-content-important" aria-hidden="true"><span class="octicon octicon-link"></span></a>Why is product content important?</h1>

<p>Product descriptions are essential for a successful online business because they help you become more visible online, share the information needed by consumers to make a buying decision, and therefore, have more sales.</p>

<h2 id="transformers">
<a class="anchor" href="#transformers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transformers</h2>

<p>The paper <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> introduces a novel architecture called Transformer. As the title indicates, it uses the attention mechanism. The focus of the original research was on translation tasks. This was followed by the introduction of several influential models, including GPT, BERT, GPT-2, DistilBERT, BART, T5, and GPT-3. This list is far from comprehensive, and is just meant to highlight a few of the different kinds of Transformer models.</p>

<p>Like LSTM, Transformer is an architecture for transforming one sequence into another one with the help of two parts (Encoder and Decoder), but it differs from the previously existing sequence-to-sequence models because it does not imply any Recurrent Networks (GRU, LSTM, etc.). The Encoder takes the input sequence and maps it into a higher dimensional space (n-dimensional vector). That abstract vector is fed into the Decoder which turns it into an output sequence. The team presenting the paper proved that architecture with only attention mechanisms without any RNN (Recurrent Neural Networks) can improve the results in translation tasks and other tasks.</p>

<h2 id="what-is-gpt-2">
<a class="anchor" href="#what-is-gpt-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is GPT-2</h2>

<p><a href="https://openai.com/blog/better-language-models/">GPT-2</a> is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.</p>

<p>GPT-2 displays a broad set of capabilities, including the ability to generate conditional synthetic text samples of unprecedented quality, where we prime the model with input and have it generate a lengthy continuation. In addition, GPT-2 outperforms other language models trained on specific domains (like Wikipedia, news, or books) without needing to use these domain-specific training datasets. On language tasks like question answering, reading comprehension, summarization, and translation, GPT-2 begins to learn these tasks from the raw text, using no task-specific training data. While scores on these downstream tasks are far from state-of-the-art, they suggest that the tasks can benefit from unsupervised techniques, given sufficient (unlabeled) data and computing.</p>

<h2 id="what-is-gpt2-home">
<a class="anchor" href="#what-is-gpt2-home" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is GPT2-Home?</h2>

<p><a href="https://huggingface.co/HamidRezaAttar/gpt2-product-description-generator">GPT2-Home</a> is a fine-tuned version of GPT-2 on Amazon home products metadata. It can generate descriptions for your <strong>home</strong> products by simply getting a text prompt. It has been fine-tuned on various types of home products to cover all types of product descriptions you need. It can generate descriptions from main home products like the furniture to home decorative products like Wall Decor, Candle, Posters and etc.</p>

<h2 id="fastai-with-transformers">
<a class="anchor" href="#fastai-with-transformers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fastai with ü§óTransformers</h2>
<p>GPT2-Home is finetuned using <a href="https://www.fast.ai/">fastai</a> and <a href="https://huggingface.co/docs/transformers/index">transformers</a>. fastai is a deep learning library that provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches.</p>

<p>Hugging Face, a company with open-source NLP technologies, provides thousands of pre-trained models to perform tasks on different modalities such as text, vision, and audio.</p>

<h2 id="-future-improvements">
<a class="anchor" href="#-future-improvements" aria-hidden="true"><span class="octicon octicon-link"></span></a>üìë Future Improvements</h2>

<p>Adding 14 new categories of products besides the only home products.</p>

  </div><a class="u-url" href="/blog/markdown/2022/02/17/gpt2-home.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Attar AI</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/HamidRezaAttar" target="_blank" title="HamidRezaAttar"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
